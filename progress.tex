\documentclass[a4paper, 11pt]{article}
\usepackage{float,fullpage,enumitem,hyperref,graphicx,url}
\graphicspath{{figures/}}

\title{Some Progress on Robot Grasping}
\author{Daniel Seita}
\date{\today}

\begin{document}
\maketitle
\tableofcontents

\clearpage
\section{The Regression Problem}

The goal here is, given information about a grasp based on two gripping locations to predict the
resulting expected or probability of the Ferrari-Canny metric (EFC and PFC, respectively). Before
delving into the details of the problem, I first describe observations on the data.

I had to remove some data points that had certain parameter values as NaN or below $-1 \times
10^{100}$. After my preprocessing, Figures~\ref{fig:efc} and~\ref{fig:pfc} show the histogram of
points for EFC and PFC respectively.  Both of these are right-skewed, with many elements closer to
zero than one, and the effect is more pronounced for the EFC data. The histogram for PFC is
interesting due to three bins having fewer elements compared to surrounding bins, but it is unlikely
to be important.

\begin{figure}[h]
  \centering
  \begin{minipage}{.48\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{fig_efc}
    \caption{The EFC data.}
    \label{fig:efc}
  \end{minipage}\hfill
    \begin{minipage}{.48\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{fig_pfc}
    \caption{The PFC data.}
    \label{fig:pfc}
  \end{minipage}
\end{figure}

Some more data statistics, for EFC:

\begin{itemize}[noitemsep]
    \item Total elements: 302205
    \item Minimum: 0.0 (occurs 28951 times)
    \item Maximum: 0.0444
    \item Mean: 0.0139
    \item Median: 0.0124
    \item Standard deviation: 0.0117
\end{itemize}

Similarly, here are the PFC statistics:

\begin{itemize}[noitemsep]
    \item Total elements: 302205
    \item Minimum: 0.0098
    \item Maximum: 0.9510
    \item Mean: 0.2994
    \item Median: 0.2647
    \item Standard deviation: 0.2422
\end{itemize}

My conclusion seems to be that running the regression on PFC for now should be fine, and if
regression works on PFC, it will work on EFC because the data distribution has the same shape. It is
mainly the raw scale of features that differs so much.

\begin{figure}[t]
\begin{center}
\begin{tabular}{c@{}c@{}c}
\includegraphics[width=0.33\linewidth]{fig_mean} &
\includegraphics[width=0.33\linewidth]{fig_std} &
\includegraphics[width=0.33\linewidth]{fig_allpoints} \\
(a) $L = -0.01094, H = 1.57817$ &
(b) $L = 0.00006, H = 0.89950$ &
(c) $L = -0.99999, H = 3.14051$ \\
\end{tabular}
\end{center}
\caption{Plots (a) and (b) are histograms of the mean and standard deviation of the 1587 features in
the data (after preprocessing to remove NaNs, etc., but not standardization). Figure (c) plots the
histogram of the raw data's $302205\times 1587$ points. For all three figures, the $H$ and $L$
indicate the highest and lowest points for those respective histograms.}
\label{fig:more_data_stats}
\end{figure}

Next, I inspect the \emph{feature} values (\emph{not} the EFC or PFC metrics) just in case there is
something out of whack.  Figure~\ref{fig:more_data_stats} shows some more plots and statistics. The
full data has 1587 features. For each feature, I compute its mean and standard deviation. The
respective histograms are in Figure~\ref{fig:more_data_stats}(a) and
Figure~\ref{fig:more_data_stats}(b). Most features have a mean or standard deviation close to zero
(but not exactly zero), and there are a few outlier features that cause the histogram to be skewed.

I can also ``flatten'' the data matrix to get $302205\times 1587 = 479599335$ points.
Figure~\ref{fig:more_data_stats}(c) shows the distribution of all the points. Most values are close
to around zero, but the data is \emph{not} sparse; only 0.0985\% of the values in the data are zero.

Now let's do some regression using Python and a (fairly weak) computer.



\section{Raw Regression Results}

To start off, I ran regression using the following procedures:

\begin{itemize}
    \item Linear Regression (abbreviated as ``Linear Regr''), which optimizes
\[
\min_w \; \|Xw - y\|_2^2,
\]
    for training matrix $X$ and target vector $y$.
    \item Ridge Regression (abbreviated as ``Ridge ($\alpha$)''), which optimizes
\[
\min_w \; \|Xw - y\|_2^2 + \alpha \|w\|_2^2,
\]
    for training matrix $X$ and target vector $y$. For $\alpha$ values, I used $\alpha = 1e-2$
    (i.e., 0.01) and $\alpha = 1e-6$, though I also tried other values but do not report them here
    as the problem is largly insensitive to the choice of $\alpha$.

    \item Stochastic Gradient Descent Regression (abbreviated as ``SGD Regr'').
    \item Kernel Regression (abbreviated as ``Kernel Regr''), with a \emph{linear} kernel.
\end{itemize}

To evaluate those methods, I split the data I had into training and validation sets (shuffling, of
course). I train on the training data, and then test on the validation and measure the \textbf{mean
absolute difference} and \textbf{mean square error}. While the algorithms above technically optimize
the squared error (or a regularized version), it is perhaps more intuitive to discuss the absolute
error, which is why I present them both here.

To make the problem more concrete, I set aside 10,000 samples to be the validation set. Then, I
tested all the procedures discussed earlier on training set sizes of 10, 50, 100, 500, 1000, 5000,
10000, 50000, 100000, and 280000 (the last one means we are using almost all of our data). This way,
the validation set is fixed while the training set grows.

I performed two other steps. First, I \textbf{standardized} the training data $X$ (but not the
target $y$) and then ran regression. Second, I \textbf{clipped} the output to be between 0 and 1.
During regression, our algorithm will sometimes predict negative values, especially when the target
itself is really small (on the order of 0.05 or less). Clipping can never decrease our performance,
so there is little reason not to use it.

I ran this procedure three times -- with different validation sets each time -- and then averaged
them together to get one curve (for a specific method).  The results are in
Figures~\ref{fig:absdiff} and~\ref{fig:sqrdiff} for the two respective error types. These figures
also have a thin line interpolated on the plot. \textbf{Those lines show the error one would get if
one estimated the output solely based on the median or mean of the training data. In some way, this
is like random guessing, so we would like our regression to substantially outperform this baseline.}

To be precise, consider the absolute difference case. We have our training set $\{X,y\}$ of some
size (which I vary in my experiments). One (obviously bad) way to estimate the output is to compute
a median of $y$, and pick that for \emph{all} of our estimates of the validation data's PFC values.
It is well known that the solution to $\min_y \; \sum_{i=1}^n |y - y_i|$ is to set $y$ to be a
median of $\{y_1, \ldots, y_n\}$. A similar case holds for the mean square error, with the mean
rather than the median.

\begin{figure}[t]
  \centering
  \begin{minipage}{.48\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{fig_absdiff}
    \caption{Absolute difference metric.}
    \label{fig:absdiff}
  \end{minipage}\hfill
    \begin{minipage}{.48\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{fig_sqrdiff}
    \caption{Squared error difference metric.}
    \label{fig:sqrdiff}
  \end{minipage}
\end{figure}

Here are some observations:

\begin{enumerate}
    \item Up to a training set size of 1000, increasing the training set actually worsens the
    performance, but beyond that, training generally improves and, in the case of absolute
    differences, hovers at around 13\% for our best-performing cases\footnote{Take the small
    training set sizes with a grain of salt, because the data here has 1587 features, and 10-1000
    samples may not be enough to learn much.}. The best number I have in my log is a 12.86\%
    difference, so on average, we are that much off of the true PFC percentage.
    \item The linear regression and ridge regression with $\alpha = 1e-6$ are very similar and
    practically overlap for both figures, so it is difficult to see the regression. According to my
    raw logs, one needs to check about four decimal places before observing a difference.
    \item The curves follow similar shapes for the two error metrics. Therefore, for simplicity, we
    only focus on the absolute error difference in future plots for convenience.
    \item The kernel regression curve does not have any data points after 10,000 samples because it
    was causing segmentation faults on my computer (i.e., it was using more than 15-16 GB of RAM).
    \item Running one full trial to get data points (remember, I ran three) took just over 15
    minutes to run on my weak computer. Therefore, if we have strong computation power, we can
    definitely work with far more than 280,000 training instances.
    \item Stochastic gradient descent appears to run faster than the other regressors, but the
    difference is not substantial with this dataset. I only used the default SGD settings from
    \texttt{scikit-learn} so it might be possible to increase the performance with a parameter
    tweak, though I doubt it will go much lower than 13\%.
    \item In all cases except the kernel regression --- which we probably shouldn't even do --- the
    methods perform substantially better than ``random guessing,'' or always estimating using the
    median or mean.
\end{enumerate}

In the following subsections, I will investigate several properties of the algorithm and their
effect on performance. I will also investigate what happens when we remove certain features;
Figures~\ref{fig:absdiff} and~\ref{fig:sqrdiff} used \emph{all} 1587 features.

\subsection{Effect of Clipping}

\begin{figure}[t]
  \centering
  \begin{minipage}{.48\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{fig_clipping_effect}
    \caption{The effect of clipping.}
    \label{fig:clipping_effect}
  \end{minipage}\hfill
    \begin{minipage}{.48\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{fig_predictions_clipping}
    \caption{Predicted PFCs.}
    \label{fig:pred_hist}
  \end{minipage}
\end{figure}

Clipping the data means that, after we predict all the validation set values, we change all negative
values to be 0, and all values greater than one to be one. Figure~\ref{fig:clipping_effect} shows
the clipping effect on the linear regression case using 10k validation instances (i.e., the same
procedure as in Figure~\ref{fig:absdiff}), but with only five training sizes (5k, 10k, 50k, 100k,
and 280k) to avoid making the plot unreadable. The plot shows that clipping results in roughly a
0.005 improvement, or half a percentage point, for the training instances.

This may or may not be what one would have expected. We can gain some further insight by looking at
how many points had to be clipped. Figure~\ref{fig:pred_hist} plots, for the 280k training data
case, the histogram of the predictions linear regression made on the testing set. It turns out that
7.7\% are below 0, so only those values were clipped, and in general, my ``eyeball test'' showed
that those values generally did correspond to low (true) PFC values.

What is interesting is the shape of the distribution. It is \emph{left-skewed}, and peaks at around
0.5. This is unusual, as the true data distribution is \emph{right-skewed}. It might be worth
investigating why the distribution is shaped like this. And yes, this is with standardization of the
data.

In any case, clipping the results is an obvious step and one that is cheap and can't hurt so there
is no reason not to use it. I use it in future results.


\subsection{Effect of Standardization}

\begin{figure}[t]
  \centering
  \begin{minipage}{.48\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{fig_standardization_effect}
    \caption{The effect of standardization.}
    \label{fig:standardization_effect}
  \end{minipage}\hfill
    \begin{minipage}{.48\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{fig_remove_gradients}
    \caption{Removing gradient features.}
    \label{fig:remove_gradients}
  \end{minipage}
\end{figure}

In general, I will always be standardizing the training data $X$ (though not the outputs, and this
standardization does \emph{not} rely on data from validation as that would be cheating).

Figure~\ref{fig:standardization_effect} shows the effect of standardization using the training set
sizes of 5k, 10k, 50k, 100k, and 280k, on the validation set of 10k samples (same data points as
earlier). With linear regression, standardization is not beneficial; it is difficult to see but the
blue and red curves overlap each other and are accurate to about three decimal places. For ridge
regression with $\alpha = 1e-2$ (this is generally larger than what we should be doing, though) the
standardization \emph{does} help, putting the performance in line with linear regression. With
stochastic gradient regression (not shown) standardization is \emph{worse} but catches up with lots
of data. It seems like standardization does not provide much extra benefit but it doesn't hurt and
it seems logical to continue using it in the future.

\subsection{Effect of Feature Removal}

It may be the case that with 1587 features, one can remove features and it will not hurt the
regression. One of our features are the gradients at the two contact points (called \texttt{gradx}
and \texttt{grady} internally). Removing both of those features, for the two contact points,
resulted in a 911-dimensional feature vector.

Figure~\ref{fig:remove_gradients} shows that, rather surprisingly, removing all those features does
not seem to harm regression -- if anything it seems to \emph{help}, and these results are consistent
with other trials not shown here (for space and Daniel Seita's time constraints). In the figure,
``Part'' indicates that we are only using part of the feature set, i.e., all features other than
gradients.

For linear regression, avoiding the gradients results in a slight increase in performance though the
difference is not great since the red and blue curves almost coincide in the plot (no gradients
means 0.1330 versus 0.1333 with gradients). For stochastic gradient descent, however, avoiding
gradients results in a far greater relative performance improvement. But then again, we would rather
be doing linear regression anyway so perhaps this is not that beneficial and may be detrimental in
the long run?

I ran these on the \emph{same} validation and training set elements\footnote{Though clearly, the
actual elements are different (911-dimensional vs 1587-dimensional), but the \emph{indices} are the
same.} to make comparison consistent. It seems like the gradients are not that helpful, and a
reduction from 1587 to 911 without loss in quality is nice. \textbf{While I will likely keep using
the full feature set in the future, I should keep this in mind in case we need performance
speed-ups.}



\section{Regression for Different Types of Grasps}

I now switch the attention towards another interesting question: whether the quality of the
regression varies according to the type of grasp. In other words, we ``bucket'' the PFC values into
several bins to see how the regression performs on each of those buckets. This is an interesting
experiment because one might think that the regression only works on the low quality grasps (PFC
close to zero) because there are so many of those.

\begin{figure}[t]
\begin{center}
\begin{tabular}{c@{}c@{}c}
\includegraphics[width=0.33\linewidth]{fig_thirds_first} &
\includegraphics[width=0.33\linewidth]{fig_thirds_second} &
\includegraphics[width=0.33\linewidth]{fig_thirds_third} \\
(a) The first third. &
(b) The second third. &
(c) The third third. \\
\end{tabular}
\end{center}
\caption{Performance of three algorithms on three parts of the data. The first, second, and third
``thirds'' are (mutually disjoint, but exhaustive) subsets of the full validation dataset, with
lower PFC values in the first third, and higher PFC values in the last third. Training set sizes
were 10k, 50k, 100k, and 280k. \textbf{Be sure to look carefully at the y-axis values.}}
\label{fig:thirds}
\end{figure}

I decided to break up the data into thirds. The procedure was as follows. For the 10,000 validation
cases, I first did the predictions as normal. Then, I computed the 33rd and 66th percentiles of the
\emph{validation} set's values (i.e., those are the ones fixed, \emph{not} the ones from the
regression). For each element in the validation set, I assigned it to be in the first, middle, or
last third based on its PFC value (lower means first third, higher means last third). Then I
computed the average absolute difference metric again, but limited the focus to the thirds.

Figure~\ref{fig:thirds} shows the performance on the three ``thirds'' of the data, using training
set sizes of 10k, 50k, 100k, and 280k. The stochastic gradient regression is a bit inconsistent
(makes sense due to the nature of the algorithm), but for linear and ridge regression, it is clear
that \textbf{they perform best on the middle third of the data, and worst on the last third of the
data}. Therefore, the higher the PFC, the more inaccurate our regression will be, and in a way that
makes sense, because from Figure~\ref{fig:pfc}, the true distribution is right-skewed so there is
not much data in the high PFC areas.  Figure~\ref{fig:pred_hist} also shows that our algorithms
generally do not predict any values higher than 0.6.

I think the algorithm would improve on the last third if we had more balanced training data.



\section{Support Vector Regression}

\begin{figure}[t]
\begin{center}
\begin{tabular}{c@{}c@{}c}
\includegraphics[width=0.49\linewidth]{fig_absdiff_svm_jan14} &
\includegraphics[width=0.49\linewidth]{fig_sqrdiff_svm_jan14} \\
(a) Mean absolute difference. &
(b) Mean squared difference. \\
\end{tabular}
\end{center}
\caption{Performance of linear regression, SGD regression, and SVM regression (using RBF kernel) on
a validation dataset of 10,000 elements. The evaluation is based on mean absolute difference (a) or
mean squared difference (b). Again, look carefully at the y-axis values. The thin blue lines
interpolated on the plots are the equivalent of random guessing. The training set sizes: 2000, 5000,
10000, 20000, 50000, 75000, 100000, and 280000.}
\label{fig:svr_results}
\end{figure}

\begin{figure}[t]
\begin{center}
\begin{tabular}{c@{}c@{}c}
\includegraphics[width=0.33\linewidth]{fig_preds_lin} &
\includegraphics[width=0.33\linewidth]{fig_preds_sgd} &
\includegraphics[width=0.33\linewidth]{fig_preds_svm} \\
(a) Linear regression. &
(b) SGD regression. &
(c) SVM regression (RBF). \\
\end{tabular}
\end{center}
\caption{Histograms of the predicted values for linear regression, SGD regression, and SVM
regression (using RBF kernel) on a validation dataset of 10000 elements. The SVM regression results
in a notable difference in the histogram and is more in line with Figure~\ref{fig:pfc}, explaining
its stronger performance. Also, note that SVM only uses 30k samples.}
\label{fig:svr_histograms}
\end{figure}

I'm surprised I didn't try this out earlier, but it looks like \textbf{Support Vector Regression}
may be the best way to regress on this data. SVM regression is similar to kernel ridge regression
since both use kernels, but they optimize different loss functions. The \texttt{sklearn}
documentation (and other sources I've looked at) suggest that SVM regression and kernel ridge
regression will have similar performance, so I didn't bother testing with SVM regression ... until
now.

My results are in Figure~\ref{fig:svr_results}, comparing ridge regression, SGD regression, and SVM
regression, with the SVM using a \emph{Radial Basis Function (RBF)} kernel. I generated this figure
in a similar manner as I did for Figures~\ref{fig:absdiff} and~\ref{fig:sqrdiff}. The validation set
was still size 10000, and I varied the number of training set elements, but did not use the largest
value (280000) for SVM regression due to computational limitations. The SVM regression lets me get
down to just below 0.1 off of the true PFC, on average. Also, note that I only used 911 features,
i.e., I got rid of the gradient features.

Some observations:

\begin{enumerate}
    \item SVM regression easily performs better than the other two algorithms, and in fact, judging
    by the shape of its curves, its performance still has not leveled off and would improve with
    more training data. \textbf{With 100,000 training instances, we can get a mean absolute error of
    0.0964 on the testing set (of 10,000 instances)}.

    \item Even with a 911-dimensional feature vector (I got rid of the gradient features), SVM
    regression takes a long time. I got similar results with the full feature vector (for the RBF
    kernel), and also got worse results with the polynomial kernel for SVM regression (results not
    shown here), though I have not rigorously tested the former claim.

    \item The mean absolute difference for SVM regression is \emph{consistent} among all three
    ``thirds,'' which is \emph{unlike} the other algorithms, which struggle at the high probability
    PFC points. (These results are not explicitly shown here.)

    \item Figure~\ref{fig:svr_histograms} shows why the SVM regression performs better, and why it
    may be better at predicting high PFC values. These show the histogram of predicted values from
    the three algorithms. The SVM regression predicts lots of values closer to zero (but mostly
    positive) and the distribution \emph{generally decreases} off. As Figure~\ref{fig:pfc}
    indicates, this follows the similar shape as the true distribution of the data. For some reason,
    the other two algorithms have histograms that \emph{increase} to about 0.5 and then level off
    sharply. They are not learning the correct shape of distributions, and their MAE stagnates at
    around 0.13.

    \item Generating Figure~\ref{fig:svr_results} took roughly seventeen hours of computational time
    on my laptop, mainly because of SVM regression.  The time it took for ridge regression and SGD
    regression was negligible in comparison. For instance, running \emph{just} the 100k training
    samples case for SVM regression took eleven and a half hours.
\end{enumerate}

The result of this section seems clear: use SVM regression with an RBF kernel, on the reduced
feature set (though I think it should be OK with the full feature set).




\section{Neural Networks for Regression}

I finally got some neural network results, and it looks like neural networks might be a viable
alternative to SVM regression, and it might even be faster. I implemented one using the
\texttt{theano} Python library. My network has one hidden layer of 1000 nodes, and \emph{one} output
node, which outputs the predicted P(FC) metric. That output node is processed with the sigmoid
function. It sure is convenient that the sigmoid's output is $[0,1]$, exactly the range of P(FC).
The network is fully connected -- I am not using convolutional ones for now. I think we need a
better feature set before we can use the convolutional ones.

But anyway, \textbf{using a training size of 100k, I can get the validation (size 10k as usual) MAE
to 0.10893}. This is almost as good as the value of 0.0964 I reported earlier for the SVM regression
with an RBF kernel, but the difference is that with a good GPU, doing the neural network code is
\emph{much} faster -- it took less than 11 minutes to do that, compared to 11.5 hours with the same
SVM regression code.

With 280k training data points, I can get the error down to \textbf{0.10296}, but to get
substantially better results, I probably need (1) a new architecture, or more importantly, (2)
better data!





\section{New Data (February 2016 edition)}

\begin{figure}[t]
  \centering
  \includegraphics[width=1\textwidth]{fig_new_histograms_pfc_vfc}
  \caption{The updated PFC/VFC data histograms.}
  \label{fig:updated_pfc_vfc}
\end{figure}

Whew, as of the beginning of February, 2016, I have updated data. The changes, as far as I am aware,
are:

\begin{itemize}
    \item Object IDs, i.e., which object was used for the grasp. I don't think this is going to be
    particularly useful to us now but we can always use it later in case we want to keep certain
    objects out of our training (or testing) data.
    \item The surface normals, so that we can identify directions appropriately (i.e., earlier we
    didn't have a sense of where these patches were located, so they could have been really close to
    each other, which is normally a bad grasp).
    \item Instead of the expected Ferrari-Canny metric \textbf{EDIT (02/15/2016): This is
    Probability of Force Closure, actually ... sorry, I'd been confused on what metric we were
    using.}, we are now using the \emph{variance} of it, which must rely on knowing the distribution
    of the P(FC), I suppose.
    \item Instead of just one P(FC) (and one V(FC)), we have three different versions, numbered
    0.05, 0.1, and 0.2. I'm assuming these represent a measure of uncertainty, so the lower the
    number (or the higher, not sure?) the less uncertainty in the measurement.
\end{itemize}

These are low-dimensional features and therefore will not substantially increase the runtime of
regression. In all, the \textbf{data is of dimension 1598}.

What's interesting is that the distribution of P(FC)s seems to be different from last time.
Figure~\ref{fig:updated_pfc_vfc} shows six subplots with the six histograms of the data (note that
the vertical axis is not the same across all subplots). There seem to be a lot more P(FC)s closer to
one now, almost as if the data were flipped from before. Well, except for the 0.2 version of P(FC).
With respect to V(FC), its distributions will likely be tough for regression to learn even after we
clip the output to be in $[0,0.25]$.

The mean of the values in the six sub-plots, in order (starting from the top row, going right,
then the bottom row):

\begin{verbatim}
[ 0.82980355  0.7332635   0.57711644  0.10364071  0.14597935  0.19168528]
\end{verbatim}

Now let's discuss the regression results. For now, assume we are using P(FC) of 0.1 as our output.

\subsection{Ridge, SVM, and Neural Network Regression on the New Data}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.6\textwidth]{fig_new_data_mae}
  \caption{The MAE for three different algorithms on training sets of various sizes.}
  \label{fig:new_data_mae}
\end{figure}

I ran regression using all the features on a subset of the data. Figure~\ref{fig:new_data_mae} shows
the MAE plot for three algorithms (ridge regression with $\alpha = 0.001$, SVM kernel regression
with an RBF kernel, and a fully connected neural network with one hidden layer of 1000 units, and
the tanh nonlinearity. In all data points here, the validation set was a fixed held-out set of
10,000 grasps.

\subsection{Other Comments on the Classifiers and Data}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.8\textwidth]{fig_predictions_and_errors}
  \caption{Histograms of the predictions (top two) and prediction errors (bottom two) of the ridge
  regression and SVM regression algorithms, as they were in Figure~\ref{fig:new_data_mae} on their
  \emph{largest} training sets. Fortunately, both algorithms seem to learn the correct distribution
  shape (top two plots) and the errors, as expected, follow a right-skewed distribution, so most are
  actually less than 0.1 --- that's good. The neural networks data is not available.}
  \label{fig:predictions_and_errors}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=1\textwidth]{fig_new_data_thirds}
  \caption{Performance of the ridge regression and SVM regression (as they were in
  Figure~\ref{fig:new_data_mae}) on the different ``thirds'' of the data. I split the validation set
  equally into thirds based on the true P(FC) values. \textbf{Notice that the y-axis labels are the
  same among all subplots}, indicating that the first third (corresponding to low P(FC) in the true
  data) is harder to learn or results in weaker performance, which makes sense because most of the
  data is concentrated in the higher values of P(FC) closer to 1.}
  \label{fig:new_data_thirds}
\end{figure}

Figure~\ref{fig:predictions_and_errors} plots the predictions and the prediction residuals for the
ridge regression and SVM regression algorithms, on their largest training set sizes from
Figure~\ref{fig:new_data_mae}. (I could not figure out how to get this data from the neural networks
code but it must be possible.) Quick observations: fortunately, both algorithms learn the correct
left-skewed distribution shape of the P(FC) for the 0.05 case (see Figure~\ref{fig:updated_pfc_vfc}
for the true distribution). And also, the error residuals have a distribution that makes sense. A
handful of bad errors are causing the MAE to rise.

Figure~\ref{fig:new_data_thirds} shows these two same algorithms and how they perform on the
different ``thirds'' of the data. It's similar to Figure~\ref{fig:thirds}, except the performance is
different due to the reversed data distribution. Also, notice that the y-axis values are the same
across all three subplots.

Here's some work for the future:
\begin{enumerate}
    \item Test with output other than the pfc of 0.1.
    \item More testing with feature selection to see if we can remove useless features.
    \item Use BIDMach; the data is 28 GB and doesn't fit in RAM.
    \item Test using neural networks with two hidden layers, and also try out the ReLU
    non-linearity.
    \item Figure out how to extract predictions (EDIT: use BIDMach for this).
\end{enumerate}



\clearpage
\section{The Home Stretch (February 22 Onwards)}

I finally managed to get the data suitable for BIDMach's neural networks and random forest models. I
shuffled and split it into 20 groups of files, and it is also the case that they are divided by
``object ID'' so one object ID is not dispersed among many files\footnote{Each object ID is actually
dispersed in two files due to a quirk in how I processed the data, but don't worry, I take this into
account when I split the data into training and testing sets.}. We want this so that the testing set
consists of object IDs that we have not seen yet. Note that the object ID itself is \emph{not} a
feature; it's just used to help split the data.

I shuffled \emph{and} normalized data so that we wouldn't have to do that inside
BIDMach\footnote{I'm not sure if it's possible to shuffle data from a Files DataSource in BIDMach,
so I shuffled beforehand, and getting the correct mean and standard deviation would require a full
pass over the data. If it doesn't fit in RAM, then it makes more sense to also do that computation
beforehand.}.

Here are the options that I set for BIDMach (I'll describe what this means later in words).
\textbf{Don't worry about knowing this. It's mainly so that I remember what settings I used for
BIDMach.} The first part comes when I manually write values in my training script. The second part
is when I print \texttt{opts0.what}.

\footnotesize
\begin{verbatim}
opts0: BIDMach.networks.Net.FDSopts = BIDMach.networks.Net$FDSopts@54124706
opts0.nstart: Int = 20
opts0.nend: Int = 29
opts0.batchSize: Int = 100
opts0.reg1weight: BIDMat.FMat = 0.00010000
opts0.npasses: Int = 20
opts0.hasBias: Boolean = true
opts0.links: BIDMat.IMat = 1
opts0.lrate: BIDMat.FMat = 0.030000
opts0.texp: BIDMat.FMat = 0.30000
opts0.evalStep: Int = 53
opts0.nweight: Float = 1.0E-4
net: BIDMach.networks.layers.NodeSet = BIDMach.networks.layers.NodeSet@27b58873
opts0.nodeset: BIDMach.networks.layers.NodeSet = BIDMach.networks.layers.NodeSet@27b58873
Option Name       Type          Value
===========       ====          =====
addConstFeat      boolean       false
aopts             Opts          null
autoReset         boolean       true
batchSize         int           100
checkPointFile    String        null
checkPointInterval  float         0.0
cumScore          int           0
debug             int           0
debugMem          boolean       false
dim               int           256
dmask             Mat           null
dorows            boolean       false
doubleScore       boolean       false
dropout           float         0.5
eltsPerSample     int           500
epsilon           float         1.0E-5
evalStep          int           53
featThreshold     Mat           null
featType          int           1
fnames            List          List(<function1>, <f...
hasBias           boolean       true
initsumsq         float         1.0E-5
langevin          float         0.0
links             IMat          1
localDir          String        
lookahead         int           2
lrate             FMat          0.030000
mask              FMat          null
momentum          FMat          null
nend              int           29
nesterov          FMat          null
nmodelmats        int           0
nodeset           NodeSet       BIDMach.networks.lay...
npasses           int           20
nstart            int           20
nweight           float         1.0E-4
nzPerColumn       int           0
order             int           0
pexp              FMat          0.50000
policies          Function2[]   null
predict           boolean       false
pstep             float         0.01
putBack           int           -1
r1nmats           int           1
reg1weight        FMat          0.00010000
resFile           String        null
sample            float         1.0
sampleFiles       float         1.0
sizeMargin        float         3.0
startBlock        int           8000
targetNorm        float         1.0
targmap           Mat           null
texp              FMat          0.30000
throwMissing      boolean       false
updateAll         boolean       false
useCache          boolean       true
useDouble         boolean       false
useGPU            boolean       true
vexp              FMat          0.50000
waitsteps         int           3
\end{verbatim}
\normalsize

The main thing I change during the training trials was the \texttt{nend} variable. Here it's 29,
which means I'm using files numbered 20 through 28, so nine training files. It turns out that for
the data I had, 9 files results in 1,132,630 training points. Not bad. Also, I halved the learning
rate to 0.015 once I began using at least 11 out of the 20 training files. Otherwise, the settings
should be fixed across different trials (i.e., with different training set sizes).

What does this neural net look like?

\begin{itemize}
    \item It's fully connected with three layers of 500, 300, and 180 hidden units, respectively.
    \item All hidden layers use the sigmoid non-linearity (surprisingly, the ReLUs and hyperbolic
    tangents don't work that well).
    \item Aside from the hidden layers, there is an input layer with 1594 units, and a single output
    node with the predicted P(FC) value.
    \item The net has $L_1$-regularization, and uses ADAGrad during training.
    \item It does \emph{not} use dropout.
\end{itemize}

\begin{figure}[t]
  \centering
  \includegraphics[width=1\textwidth]{fig_nn_bidmach_try01}
  \caption{The MAE and training time for neural networks from BIDMach. The left uses a \emph{log}
  scale for the training instances; the right uses the \emph{raw} scale. Thus, on the left, the
  maximum point on the x-axis scale corresponds to a value of 5 million training data points (though
  we have ``only'' 2.5 million).}
  \label{fig:bidmach_try01}
\end{figure}

Figure~\ref{fig:bidmach_try01} shows the first stab at BIDMach, training for 20 passes over the data
and using a held-out test set (of unique object IDs) of 50k elements, kept consistent across all
trials. Previously, we used 10k testing points, so our test results should be less noisy. Our
training set sizes ranged from 10k to 2,270,484.

\textbf{Conclusion?} The results are clearly superior to those from Figure~\ref{fig:new_data_mae},
especially once the dataset is around a million elements. The best MAE for P(FC) that we get on the
testing set is \textbf{0.0695}, from the second-largest training set.

On another pleasant note, we see that the run time is (mostly) linear. \textbf{I'll be recording the
running time of all experiments I do in the near future}, in case we want to report this. For
\emph{predicting}, the BIDMach code can predict blazingly fast, so that's not an issue.

\subsection{Tuning the Net}

Here are some notes on performance with different settings, in an effort to get to never-before-seen
performance levels:

\begin{enumerate}
    \item Using the same network as those from Figure~\ref{fig:bidmach_try01}, and training with the
    largest training data there with 100 iterations (so it's the same as what's reported in the
    figure, but I trained it 5 times longer), \textbf{I get an MAE of 0.0631}, the best one yet. The
    training time was 7918.9 seconds, or about 131 minutes of training time, so a few hours seems
    like a reasonable upper bound for training, as I don't plan on running more than 100 iterations,
    or using much larger neural network architectures.

    \item Scratch that, we have a new MAE record. I ran for 50 iterations (just to save time, I
    won't run for that many iterations in the future ... perhaps 20 is a good value to stay with)
    but this time, I changed the architecture so that it is now an 800-400-200 network. This net got
    an \textbf{MAE of 0.0533} with a training time of 4474.1 seconds. With the same architecture but
    with 20 iterstions results in 1788.3 seconds and an MAE of 0.0710.

    \item With a slightly larger network (1000-500-250), I get an MAE of 0.0628 and a training time
    of 1800.0 seconds (20 iterations), an \textbf{MAE of 0.0548} with time 4676.8 seconds (50
    iterations) and an \textbf{MAE of 0.0503} with time 8698.5 seconds (100 iterations).
\end{enumerate}


\subsection{Random Forests}

Random forests from BIDMach also have good performance on this data.




\end{document}
